{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff4300f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corrected_tile_4_56.tiff (1/3)\n",
      "Subimages processed: 81/81\n",
      "No bounding boxes found.\n",
      "\n",
      "Processing corrected_tile_7_22.tiff (2/3)\n",
      "Subimages processed: 81/81\n",
      "Subareas filtered: 4/4\n",
      "Number of bounding boxes: 7164\n",
      "\n",
      "Processing corrected_tile_7_222.tiff (3/3)\n",
      "Subimages processed: 384/425\n",
      "Subareas filtered: 12/12\n",
      "Number of bounding boxes: 7374\n",
      "\n",
      "Job is done\n",
      "Total images processed: 3/3\n",
      "This took 0.0120 hours.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000000  # Allow large images to load\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from pyometiff import OMETIFFReader\n",
    "import sys\n",
    "from torchvision.ops import nms\n",
    "import math\n",
    "import tifffile\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Settings\n",
    "input_folder = \"F:\\\\Users\\\\labo\\\\LVerschuren\\\\gigapixelwoodbot beech scan\\\\test\"  # Folder containing input images\n",
    "output_folder = \"F:\\\\Users\\\\labo\\\\LVerschuren\\\\gigapixelwoodbot beech scan\\\\seg\"  # Folder for output images\n",
    "model_weights = 'D:\\\\Users\\\\labo\\\\Documents\\\\JanVdB\\\\yolo code\\\\segment\\\\train7\\\\weights\\\\best.pt' # file containing the best model weights\n",
    "object_class = 1 # indices of class to segment for example 0 (RAYS), 1 (VESSELS): 0 if only one class in the model\n",
    "sub_image_size = 640  # User-defined sub-image size, should be 640 for YOLOv8\n",
    "overlap_percent = 0.40  # User-defined overlap, 0.1 = 10% overlap\n",
    "confidence = 0.3  # (0.4 vessels, 0.3 rays) Sets the minimum confidence threshold for detections. Objects detected with confidence below this threshold will be disregarded. Adjusting this value can help reduce false positives.\n",
    "IntersectionOverUnion = 0.8  # Intersection Over Union (IoU) threshold for Non-Maximum Suppression (NMS). Higher values result in fewer detections by eliminating overlapping boxes, useful for reducing duplicates.\n",
    "saveExtraImages = False # if True it will save the original image with bounding boxes and masks (takes a lot of RAM for large images!) \n",
    "countBoxes = True # if True it will calculate the number of bounding boxes to count the number of detected objects\n",
    "\n",
    "\n",
    "# Initialize counters\n",
    "total_images_processed = 0\n",
    "total_subimages_processed = 0\n",
    "\n",
    "def process_sub_image(sub_image, yolo_model, full_mask, confidence, IntersectionOverUnion, x, y, sub_image_size, bounding_boxes):\n",
    "    global total_subimages_processed\n",
    "    try:\n",
    "        # Run YOLOv8 prediction on the sub-image\n",
    "        results = yolo_model.predict(sub_image, save=False, save_txt=False, save_conf=False, conf=confidence, iou=IntersectionOverUnion, verbose=False, retina_masks = True)\n",
    "\n",
    "        # Check if any objects were detected\n",
    "        if results and len(results) > 0:\n",
    "            masks = results[0].masks.data\n",
    "            boxes = results[0].boxes.data\n",
    "            clss = boxes[:, 5]\n",
    "            vessel_indices = torch.where(clss == object_class)\n",
    "            vessel_masks = masks[vessel_indices]\n",
    "            vessel_masks = torch.any(vessel_masks, dim=0).int() * 255\n",
    "            vessel_mask = np.array(vessel_masks.cpu())\n",
    "\n",
    "            if countBoxes == True:\n",
    "                # Extract bounding boxes for the class\n",
    "                for box in boxes[vessel_indices]:\n",
    "                    x1, y1, x2, y2, conf, cls = box\n",
    "                    # Check if the bounding box is not at the edge of the sub-image\n",
    "                    if x1 > 1 and y1 > 1 and x2 < sub_image_size-1 and y2 < sub_image_size-1:\n",
    "                        bounding_boxes.append([x + int(x1), y + int(y1), x + int(x2), y + int(y2), conf.item()])\n",
    "            else: \n",
    "                for box in boxes[vessel_indices]:\n",
    "                    x1, y1, x2, y2, conf, cls = box\n",
    "                    bounding_boxes.append([x + int(x1), y + int(y1), x + int(x2), y + int(y2), conf.item()])\n",
    "                    \n",
    "        else:\n",
    "            vessel_mask = np.zeros((sub_image_size, sub_image_size))\n",
    "\n",
    "    except Exception:\n",
    "        vessel_mask = np.zeros((sub_image_size, sub_image_size))\n",
    "\n",
    "    roi = full_mask[y:y+sub_image_size, x:x+sub_image_size]\n",
    "    overlay_mask = vessel_mask[:, :]\n",
    "    final_image = np.maximum(roi, overlay_mask)\n",
    "    full_mask[y:y+sub_image_size, x:x+sub_image_size] = final_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_nms_in_windows(bounding_boxes, img_width, img_height, window_size, IntersectionOverUnion, overlap_percent):\n",
    "    window_size = int(window_size * 10.5)  # bigger window size\n",
    "    \n",
    "    numx = math.ceil((img_width+50)/(window_size*(1-overlap_percent*1.5)))\n",
    "    stridex = (img_width+50)/numx\n",
    "    numy = math.ceil((img_height+50)/(window_size*(1-overlap_percent*1.5)))\n",
    "    stridey = (img_height+50)/numy\n",
    "\n",
    "    counterAreas = 0\n",
    "    totalAreas = numx * numy\n",
    "    sys.stdout.write(f\"\\nSubareas filtered: {counterAreas}/{totalAreas}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for y in np.linspace(-25, img_height+25, num = numy):\n",
    "        for x in np.linspace(-25, img_width+25, num = numx):\n",
    "            # Define window bounds\n",
    "            x_min, y_min = x, y\n",
    "            x_max, y_max = x + window_size, y + window_size\n",
    "            \n",
    "            # Collect boxes in the window (including boxes that cross the window boundary)\n",
    "            window_boxes = [box for box in bounding_boxes if not (box[2] < x_min or box[0] > x_max or box[3] < y_min or box[1] > y_max)]\n",
    "\n",
    "            # Remove these boxes from the original list\n",
    "            bounding_boxes = [box for box in bounding_boxes if (box[2] < x_min or box[0] > x_max or box[3] < y_min or box[1] > y_max)]\n",
    "            \n",
    "            if window_boxes:\n",
    "                bbox_array = np.array(window_boxes)\n",
    "                bbox_coords = bbox_array[:, :4]\n",
    "                bbox_scores = bbox_array[:, 4]\n",
    "\n",
    "                # Apply Non-Maximum Suppression\n",
    "                keep_indices = nms(torch.tensor(bbox_coords), torch.tensor(bbox_scores), 1 - IntersectionOverUnion)\n",
    "                bounding_boxes.extend([window_boxes[i] for i in keep_indices])\n",
    "\n",
    "            # counter\n",
    "            counterAreas += 1\n",
    "            sys.stdout.write(f\"\\rSubareas filtered: {counterAreas}/{totalAreas}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    # Remove duplicates\n",
    "    bounding_boxes = np.array(bounding_boxes)\n",
    "    _, unique_indices = np.unique(bounding_boxes[:, :4], axis=0, return_index=True)\n",
    "    bounding_boxes = bounding_boxes[unique_indices]\n",
    "    \n",
    "    return bounding_boxes.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load YOLOv8 model\n",
    "    model = YOLO(model_weights)\n",
    "\n",
    "    # List of files to process\n",
    "    file_list = [f for f in os.listdir(input_folder) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".tif\", \".tiff\", \".ome.tif\"))]\n",
    "    total_images = len(file_list)\n",
    "\n",
    "    # Process multiple images\n",
    "    for idx, filename in enumerate(file_list):\n",
    "        print(f\"\\nProcessing {filename} ({idx+1}/{total_images})\")\n",
    "        input_image_path = os.path.join(input_folder, filename)\n",
    "        if filename.lower().endswith((\".ome.tif\")):\n",
    "            reader = OMETIFFReader(input_image_path)\n",
    "            img, metadata, xml_metadata = reader.read()\n",
    "#            img = Image.fromarray(img)   \n",
    "            img_width, img_height = img.shape[1], img.shape[0]\n",
    "        else:\n",
    "            img = Image.open(input_image_path)\n",
    "            img_width, img_height = img.size\n",
    "            \n",
    "        full_mask = np.zeros((img_height, img_width), dtype=np.uint8) # Initialize full binary mask\n",
    "        bounding_boxes = []\n",
    "\n",
    "        overlap = int(sub_image_size * overlap_percent)\n",
    "        stride = sub_image_size - overlap\n",
    "\n",
    "        # Adjust stride to fit within image dimensions\n",
    "        stride_x = min(stride, img_width - sub_image_size)\n",
    "        stride_y = min(stride, img_height - sub_image_size)\n",
    "\n",
    "        total_subimages = ((img_height // stride_y) * (img_width // stride_x)) + (img_width // stride_x) + (img_height // stride_y) + 1\n",
    "        subimages_done = 0\n",
    "\n",
    "        for y in range(0, img_height - sub_image_size + 1, stride_y):\n",
    "            for x in range(0, img_width - sub_image_size + 1, stride_x):\n",
    "                box = (x, y, x + sub_image_size, y + sub_image_size)\n",
    "                left, upper, right, lower = box\n",
    "                if filename.lower().endswith((\".ome.tif\")):\n",
    "                    sub_img = img[upper:lower, left:right] # img[upper:lower, left:right]\n",
    "                    sub_img = Image.fromarray(sub_img)\n",
    "                else:\n",
    "                    sub_img = img.crop(box) # img.crop((left, upper, right, lower))\n",
    "                process_sub_image(sub_img, model, full_mask, confidence, IntersectionOverUnion, x, y, sub_image_size, bounding_boxes)\n",
    "                subimages_done += 1\n",
    "                sys.stdout.write(f\"\\rSubimages processed: {subimages_done}/{total_subimages}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # pass over bottom row\n",
    "        for x in range(0, img_width - sub_image_size + 1, stride_x):\n",
    "            box = (x, img_height - sub_image_size, x + sub_image_size, img_height)\n",
    "            left, upper, right, lower = box\n",
    "            if filename.lower().endswith((\".ome.tif\")):\n",
    "                sub_img = img[upper:lower, left:right] # img[upper:lower, left:right]\n",
    "                sub_img = Image.fromarray(sub_img)\n",
    "            else:\n",
    "                sub_img = img.crop(box)\n",
    "            process_sub_image(sub_img, model, full_mask, confidence, IntersectionOverUnion, x, img_height - sub_image_size, sub_image_size, bounding_boxes)\n",
    "            subimages_done += 1\n",
    "            sys.stdout.write(f\"\\rSubimages processed: {subimages_done}/{total_subimages}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # pass over right row \n",
    "        for y in range(0, img_height - sub_image_size + 1, stride_y):\n",
    "            box = (img_width - sub_image_size , y, img_width, y + sub_image_size)\n",
    "            left, upper, right, lower = box\n",
    "            if filename.lower().endswith((\".ome.tif\")):\n",
    "                sub_img = img[upper:lower, left:right] # img[upper:lower, left:right]\n",
    "                sub_img = Image.fromarray(sub_img)\n",
    "            else:\n",
    "                sub_img = img.crop(box)\n",
    "            process_sub_image(sub_img, model, full_mask, confidence, IntersectionOverUnion, img_width - sub_image_size , y, sub_image_size, bounding_boxes)\n",
    "            subimages_done += 1\n",
    "            sys.stdout.write(f\"\\rSubimages processed: {subimages_done}/{total_subimages}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # pass over right bottom corner\n",
    "        box = (img_width - sub_image_size , img_height - sub_image_size, img_width, img_height)\n",
    "        left, upper, right, lower = box\n",
    "        if filename.lower().endswith((\".ome.tif\")):\n",
    "            sub_img = img[upper:lower, left:right] # img[upper:lower, left:right]\n",
    "            sub_img = Image.fromarray(sub_img)\n",
    "        else:\n",
    "            sub_img = img.crop(box)\n",
    "        process_sub_image(sub_img, model, full_mask, confidence, IntersectionOverUnion, img_width - sub_image_size, img_height - sub_image_size, sub_image_size, bounding_boxes)\n",
    "        subimages_done += 1\n",
    "        sys.stdout.write(f\"\\rSubimages processed: {subimages_done}/{total_subimages}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # write mask\n",
    "        mask_output_path = os.path.join(output_folder, f\"mask_{filename}\")\n",
    "        cv2.imwrite(mask_output_path, full_mask)\n",
    "\n",
    "        \n",
    "        # After processing all sub-images Convert the bounding boxes to a format suitable for NMS\n",
    "\n",
    "        if countBoxes == True:\n",
    "            if len(bounding_boxes) > 0:\n",
    "                \n",
    "                # Save unfiltered bounding boxes\n",
    "                bbox_output_path = os.path.join(output_folder, f\"unfiltered_bboxes_{filename}.txt\")\n",
    "                with open(bbox_output_path, 'w') as f:\n",
    "                    for bbox in bounding_boxes:\n",
    "                        f.write(f\"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]},{bbox[4]}\\n\")\n",
    "            \n",
    "                # Apply Non-Maximum Suppression as a moving window\n",
    "                filtered_bounding_boxes = apply_nms_in_windows(bounding_boxes, img_width, img_height, sub_image_size, IntersectionOverUnion, overlap_percent)\n",
    "    \n",
    "                # print the ammount of bounding boxes\n",
    "                print(f\"\\nNumber of bounding boxes: {len(filtered_bounding_boxes)}\")\n",
    "            \n",
    "                # Save filtered bounding boxes\n",
    "                bbox_output_path = os.path.join(output_folder, f\"filtered_bboxes_{filename}.txt\")\n",
    "                with open(bbox_output_path, 'w') as f:\n",
    "                    for bbox in filtered_bounding_boxes:\n",
    "                        f.write(f\"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]},{bbox[4]}\\n\")\n",
    "                \n",
    "            else: \n",
    "                print(\"\\nNo bounding boxes found.\")\n",
    "\n",
    "        if saveExtraImages == True:\n",
    "            # convert to np\n",
    "            if not filename.lower().endswith((\".ome.tif\")):\n",
    "                img = np.array(img)\n",
    "            full_mask = np.array(full_mask)\n",
    "\n",
    "            # Ensure both img and full_mask are of type uint8\n",
    "            img = img.astype(np.uint8)\n",
    "            full_mask = full_mask.astype(np.uint8)\n",
    "            \n",
    "            # Convert the mask to 3 channels\n",
    "            background = np.zeros_like(full_mask)\n",
    "            full_mask = cv2.merge([full_mask, background, background])\n",
    "\n",
    "            # Discard the alpha channel in original image if present\n",
    "            if img.shape[2] == 4:\n",
    "                img = img[:, :, :3]\n",
    "\n",
    "            # Overlay the mask on the image\n",
    "            overlay = cv2.addWeighted(img, 1, full_mask, 0.3, 0)\n",
    "            # Write image with mask\n",
    "            ImageAndMask_output_path = os.path.join(output_folder, f\"imageAndMask_{filename}\")\n",
    "            cv2.imwrite(ImageAndMask_output_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)) \n",
    "\n",
    "            if countBoxes == True: \n",
    "                if len(bounding_boxes) > 0:\n",
    "                    # Ensure img is contiguous\n",
    "                    if not img.flags['C_CONTIGUOUS']:\n",
    "                        img = np.ascontiguousarray(img)\n",
    "                \n",
    "                    # Draw bounding boxes on the original image\n",
    "                    for bbox in filtered_bounding_boxes:\n",
    "                        x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "                        \n",
    "                        # Clip the bounding box coordinates to be within the image dimensions\n",
    "                        x1 = np.clip(x1, 0, img.shape[1] - 1)\n",
    "                        y1 = np.clip(y1, 0, img.shape[0] - 1)\n",
    "                        x2 = np.clip(x2, 0, img.shape[1] - 1)\n",
    "                        y2 = np.clip(y2, 0, img.shape[0] - 1)\n",
    "                        \n",
    "                        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "            \n",
    "                    # Write image with bounding boxes\n",
    "                    if countBoxes == True:\n",
    "                        ImageAndBoxes_output_path = os.path.join(output_folder, f\"imageAndBoxes_{filename}\")\n",
    "                        cv2.imwrite(ImageAndBoxes_output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "                \n",
    "\n",
    "        # Update the image counter\n",
    "        total_images_processed += 1\n",
    "\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    # Calculate elapsed time in hours\n",
    "    elapsed_time_in_hours = (end_time - start_time) / 3600\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\nJob is done\")\n",
    "    print(f\"Total images processed: {total_images_processed}/{total_images}\")\n",
    "    print(f\"Elapsed Time: {elapsed_time_in_hours:.3f} hours.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two masks\n",
    "\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000000  # Allow large images to load\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tifffile as tiff\n",
    "\n",
    "def load_mask(file_path):\n",
    "    # Load the binary mask as a grayscale image\n",
    "    mask = Image.open(file_path).convert('L')\n",
    "    mask_np = np.array(mask)\n",
    "    # Ensure the mask is binary\n",
    "    _, binary_mask = cv2.threshold(mask_np, 127, 255, cv2.THRESH_BINARY)\n",
    "    return binary_mask\n",
    "\n",
    "def merge_masks(mask1, mask2):\n",
    "    # Create an empty color image\n",
    "    combined_image = np.zeros((mask1.shape[0], mask1.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add cyan color to the first mask\n",
    "    combined_image[mask1 > 0] = [0, 255, 255]  # Cyan color (R, G, B order in Pillow)\n",
    "    \n",
    "    # Add red color to the second mask\n",
    "    combined_image[mask2 > 0] = [255, 0, 0]  # Red color\n",
    "    \n",
    "    # Where both masks overlap, make it purple (red + cyan)\n",
    "    overlap = np.bitwise_and(mask1, mask2)\n",
    "    combined_image[overlap > 0] = [255, 0, 255]  # Purple color\n",
    "    \n",
    "    return combined_image\n",
    "\n",
    "def save_image(image, file_path):\n",
    "    # Save the image using tifffile\n",
    "    tiff.imwrite(file_path, image)\n",
    "    print(f\"Image saved to {file_path}\")\n",
    "\n",
    "def main(mask1_path, mask2_path, output_path):\n",
    "    mask1 = load_mask(mask1_path)\n",
    "    mask2 = load_mask(mask2_path)\n",
    "    \n",
    "    combined_image = merge_masks(mask1, mask2)\n",
    "    \n",
    "    save_image(combined_image, output_path)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    mask1_path = 'F:\\\\Users\\\\labo\\\\LVerschuren\\\\gigapixelwoodbot beech scan\\\\segmented image\\\\vessels_mask_crop.tif'\n",
    "    mask2_path = 'F:\\\\Users\\\\labo\\\\LVerschuren\\\\gigapixelwoodbot beech scan\\\\segmented image\\\\rays_mask_crop.tif'\n",
    "    output_path = 'F:\\\\Users\\\\labo\\\\LVerschuren\\\\gigapixelwoodbot beech scan\\\\segmented image\\\\merged_mask_crop.ome.tif'\n",
    "    \n",
    "    main(mask1_path, mask2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713cbad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eef252-7c9d-4af2-af43-ddbf26acaec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc5234-33c3-4f62-849c-17d19f9513e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
